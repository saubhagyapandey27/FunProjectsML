{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aee60ee-6983-42b3-9d3a-1515edca720a",
   "metadata": {},
   "source": [
    "# PDF SEARCHER  ~by Saubhagya Pandey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64baa4-bdfc-4721-9ade-bc04c5622ac9",
   "metadata": {},
   "source": [
    "## Note: This searcher may give absurd results if the PDF you provide is not well-encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7abce0-5735-42fa-b587-adb44cb4e517",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354bdb34-3744-49b8-8c00-c1a991e502a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccfe74-81ec-4b2c-8f3c-b72947b3d55b",
   "metadata": {},
   "source": [
    "### Function to extract all the text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35a1ae9-05d7-4af5-b5f9-31a54e4eda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86e7ea-bdbc-41f1-8a7d-4269eda993b8",
   "metadata": {},
   "source": [
    "### Function for Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d4f91c-4b1f-4307-b868-9fc607f9a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert text to lowercase and tokenize\n",
    "    tokens= [token.strip() for token in tokens] #Removing trailing spaces\n",
    "    tokens = [token for token in tokens if token not in string.punctuation] #Removing isolated punctuations\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Join tokens back into text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30803c4-195c-4f6d-8241-a08db3689afa",
   "metadata": {},
   "source": [
    "### Function to create text chunks (list of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca85dd8-6eb7-47ef-898d-cf7db1146972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9a457-ec7d-40c9-b09b-6c3e2ac21897",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorization of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685b72a3-9383-42a0-9e86-34967147e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text_chunks):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(text_chunks) #TAKES A LIST OF STRINGS AS INPUT\n",
    "    return tfidf_vectorizer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9fd9d-6627-499d-a399-47301cf96670",
   "metadata": {},
   "source": [
    "### Function for Searching the Query (semantic search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be040330-6641-4e2d-aca7-676c8f280ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_vector, tfidf_matrix, top_n):\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    most_similar_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    return most_similar_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a991f3-4e07-4b15-8ca1-3f2e54e802bf",
   "metadata": {},
   "source": [
    "### Final Function to return the output text (gathering search results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e188be17-15af-42d3-bf3d-38c616d1b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, text_chunks, tfidf_vectorizer, tfidf_matrix, top_n=5, ans_len=50):\n",
    "    processed_query = preprocess_text(query)\n",
    "    query_vector = tfidf_vectorizer.transform([processed_query])\n",
    "    most_similar_indices = semantic_search(query_vector, tfidf_matrix,top_n)\n",
    "    answers = []\n",
    "\n",
    "    for index in most_similar_indices:\n",
    "        lst=[]\n",
    "        x=0\n",
    "        while (len(lst)<ans_len)&(len(text_chunks)>index+x):\n",
    "            lst.extend([i for i in text_chunks[index+x].strip().split()])\n",
    "            x+=1\n",
    "        answer_chunk = ' '.join(lst)\n",
    "        answers.append(answer_chunk)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d61725-ec58-4b2f-b5ce-2e17d0c8ca1a",
   "metadata": {},
   "source": [
    "### INPUTS: Path of PDF, Query & Vaiables to decide the volume of Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1afde-308b-459e-9b9f-08d61df884d2",
   "metadata": {},
   "source": [
    "PATH OF PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc417585-35ae-4aeb-9c85-354425d0a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/sample4.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15caa1-1928-4966-9c07-b620f7c7b010",
   "metadata": {},
   "source": [
    "QUERY TO BE SEARCHED FOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a186189-b11f-45ef-a9a4-99a624c65570",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'aim of author for writing this book'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd72e58-2e06-47f8-ab9d-fefd16cc0914",
   "metadata": {},
   "source": [
    "NUMBER OF SEARCH RESULTS USER WANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f5a2a5-79e4-4f85-b65c-716ceae0f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_results = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797fd97-2482-4b20-8c52-599a9f6a8178",
   "metadata": {},
   "source": [
    "MINIMUM NUMBER OF WORDS EACH SEARCH RESULT SHOUL HAVE ACCORDING TO USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e3dc768-284c-49e8-9c36-e872a7d8cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_per_result = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3fd23a-3e69-4056-a0a8-130f92e85ae8",
   "metadata": {},
   "source": [
    "### MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99e8b3a-881d-4ef3-aece-3795c3208e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_from_pdf(pdf_path) #Extracting text from pdf\n",
    "text_chunks = split_text_into_chunks(pdf_text) #Creating chunks\n",
    "processed_chunks = [preprocess_text(chunk) for chunk in text_chunks] #Pre-processing chunks\n",
    "tfidf_vectorizer, tfidf_matrix = vectorize_text(processed_chunks) #Vectorization of chunks\n",
    "answers = answer_query(query, text_chunks, tfidf_vectorizer, tfidf_matrix, num_results, len_per_result) #Storing results to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33fe4da8-dc01-4aa0-9365-0367e4ac0a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## MATCHED TEXT FOUND : [1] \n",
      "\n",
      " While I am writing this book, I am all alone. I don’t have any friends. Not even a single soul who I can hang out with. Not because I am patheti c or a loner . But because I never stayed in any school for longer than two years, I never had the kind of best friend we see in movies or books. \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [2] \n",
      "\n",
      " Books have it all. Read books and feed your brain . In your free time, or what you call ‘alone time’, take out a book and read at least 5 pages. Allow yourself to drown in the pool of words. Read books on the topics that pique your interest. Read books written by your industry expert and learn what they have learned after spending decades working. \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [3] \n",
      "\n",
      " When the idea of writing this book came to my mind, I decided that I wouldn’ t write a book that speaks data but rather a book that speaks from one heart to another heart. That’s what you need, right? You don’t care how many people in the world are lonely . \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [4] \n",
      "\n",
      " Should I stop writing then? I believe that we are beyond the titles of the external world. I think a particular word cannot define our core being. One tag cannot describe you or me. We are more than just our professional goals and personal life relationships. All these things are external. \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [5] \n",
      "\n",
      " She loves writing and reading. She lives in a world that exists beyond the human mind. She loves natur e and often finds herself gazing at the clear sky to find a hidden secret of the Universe. She loves coffee and she drinks it like it’s a drop of heaven’ I understand these things cannot be noticed by others but they can be noticed by you. \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [6] \n",
      "\n",
      " Write down one or two things that you think you don’t like and then write why you don’t like those things. It will help you to know your values, hence, getting closer to yourself a little. Not only that, but you will also understand that you are not a follower and that you don’t accept every definition that’s been thrown at you. \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [7] \n",
      "\n",
      " Because in my opinion, you cannot write if you don’t want to. Writing, to me, is not about a routine but music to my heart. You cannot force yourself to listen to music. So instead of forcing myself to write first thing in the morning, I write whenever I feel like it. \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [8] \n",
      "\n",
      " Like I don’t want people to say, ‘Renuka is a great author with excellent writing skills’ I want people to say, ‘Renuka writes in a way that makes you feel safe, loved, and understood. She loves writing and reading. She lives in a world that exists beyond the human mind. \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [9] \n",
      "\n",
      " Honestly , I have no authority to ask these questions. I wasn’ t very nice to myself either . I used to think that there was something wrong with me and that was the reason people left me. And if I can fix that ‘wrong’ thing in me, perhaps, people will stay . \n",
      "\n",
      "\n",
      "## MATCHED TEXT FOUND : [10] \n",
      "\n",
      " I am either writing or reading. If not any of them, then you might find me sleeping or eating. You can also read my first book, “The Wounds of My Words” if you like to read fiction with a mix of life lessons that can touch your heart. And if you have anything to say to me, you can reach me here: theartofbeingalone@r enukagavrani.com \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the results\n",
    "for i,answer in enumerate(answers):\n",
    "    print(f\"## MATCHED TEXT FOUND : [{i+1}]\",'\\n\\n',answer,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179a768-9b04-42e8-a8d8-24fff89e935b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
